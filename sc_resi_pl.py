{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a31bd346",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-09T22:57:55.424046Z",
     "iopub.status.busy": "2025-11-09T22:57:55.423625Z",
     "iopub.status.idle": "2025-11-09T23:05:24.592403Z",
     "shell.execute_reply": "2025-11-09T23:05:24.591153Z"
    },
    "papermill": {
     "duration": 449.174528,
     "end_time": "2025-11-09T23:05:24.594000",
     "exception": false,
     "start_time": "2025-11-09T22:57:55.419472",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage-1 Scorecard OOF AUC: 0.91118\n",
      "\n",
      "Saved submission.csv\n",
      "Final OOF AUC (post residual): 0.92095\n"
     ]
    }
   ],
   "source": [
    "# filename: scorecard_residual_pseudolabel.py\n",
    "# Kaggle S5E11 — Loan Payback\n",
    "# Radical CPU-only pipeline (≤ ~3–4 hours): Two-stage Credit Scorecard + Residual LightGBM + Extreme Test Pseudo-Labeling\n",
    "# Key ideas:\n",
    "#   1) Fold-wise WOE (Weight of Evidence) transformations for numerics + categoricals (leakage-safe)\n",
    "#   2) IV (Information Value) based feature selection to keep only high-signal WOE features\n",
    "#   3) Stage-1 base model: Logistic Regression on WOE features (classic risk scorecard)\n",
    "#   4) Stage-2 residual model: LightGBM on residuals to capture non-linearities\n",
    "#   5) Pseudo-labeling: add extreme-confidence test samples to Stage-2 training and refit\n",
    "#   6) Final isotonic calibration on OOF predictions → submission.csv\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# ------------------------------\n",
    "# Config\n",
    "# ------------------------------\n",
    "SEED = 42\n",
    "N_FOLDS = 5                    # 5 folds for speed + stability\n",
    "TARGET = \"loan_paid_back\"\n",
    "ID_COL = \"id\"\n",
    "\n",
    "NUM_COLS_BASE = [\n",
    "    \"annual_income\", \"debt_to_income_ratio\", \"credit_score\",\n",
    "    \"loan_amount\", \"interest_rate\"\n",
    "]\n",
    "CAT_COLS_BASE = [\n",
    "    \"gender\", \"marital_status\", \"education_level\",\n",
    "    \"employment_status\", \"loan_purpose\", \"grade_subgrade\"\n",
    "]\n",
    "\n",
    "LGB_PARAMS = {\n",
    "    \"objective\": \"regression\",  # residual regression\n",
    "    \"metric\": \"rmse\",\n",
    "    \"learning_rate\": 0.03,\n",
    "    \"num_leaves\": 80,\n",
    "    \"feature_fraction\": 0.9,\n",
    "    \"bagging_fraction\": 0.9,\n",
    "    \"bagging_freq\": 1,\n",
    "    \"min_data_in_leaf\": 60,\n",
    "    \"lambda_l2\": 8.0,\n",
    "    \"max_depth\": -1,\n",
    "    \"n_estimators\": 2400,\n",
    "    \"verbosity\": -1,\n",
    "    \"random_state\": SEED,\n",
    "}\n",
    "\n",
    "# Pseudo-label thresholds (extremes)\n",
    "PL_POS_TH = 0.985\n",
    "PL_NEG_TH = 0.015\n",
    "PL_MAX_RATIO = 0.20  # cap pseudo-labeled set to ≤ 20% of train size\n",
    "\n",
    "# ------------------------------\n",
    "# Data\n",
    "# ------------------------------\n",
    "def load_data():\n",
    "    train_path = \"train.csv\" if os.path.exists(\"train.csv\") else \"/kaggle/input/playground-series-s5e11/train.csv\"\n",
    "    test_path  = \"test.csv\"  if os.path.exists(\"test.csv\")  else \"/kaggle/input/playground-series-s5e11/test.csv\"\n",
    "    train = pd.read_csv(train_path)\n",
    "    test = pd.read_csv(test_path)\n",
    "    y = train[TARGET].astype(float)\n",
    "    X = train.drop(columns=[TARGET])\n",
    "    test_ids = test[ID_COL].copy()\n",
    "    return X, y, test, test_ids\n",
    "\n",
    "# ------------------------------\n",
    "# Feature Engineering helpers\n",
    "# ------------------------------\n",
    "def engineer(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "\n",
    "    # Numeric safety\n",
    "    for c in NUM_COLS_BASE:\n",
    "        if c in out.columns:\n",
    "            out[c] = pd.to_numeric(out[c], errors=\"coerce\")\n",
    "\n",
    "    # Grade parsing\n",
    "    if \"grade_subgrade\" in out.columns:\n",
    "        g = out[\"grade_subgrade\"].astype(str)\n",
    "        out[\"grade_letter\"] = g.str[0]\n",
    "        out[\"subgrade_num\"] = pd.to_numeric(g.str[1:], errors=\"coerce\").fillna(3)\n",
    "        letter_map = {\"A\":6,\"B\":5,\"C\":4,\"D\":3,\"E\":2,\"F\":1}\n",
    "        out[\"grade_letter_ord\"] = out[\"grade_letter\"].map(letter_map).fillna(3).astype(int)\n",
    "\n",
    "    # Ratios/interactions\n",
    "    if {\"loan_amount\",\"annual_income\"}.issubset(out.columns):\n",
    "        out[\"loan_to_income\"] = out[\"loan_amount\"] / (out[\"annual_income\"] + 1.0)\n",
    "        out[\"log_loan_to_income\"] = np.log1p(out[\"loan_to_income\"].clip(lower=0))\n",
    "    if {\"interest_rate\",\"debt_to_income_ratio\"}.issubset(out.columns):\n",
    "        out[\"interest_burden\"] = out[\"interest_rate\"] * out[\"debt_to_income_ratio\"]\n",
    "    if \"credit_score\" in out.columns:\n",
    "        cs = out[\"credit_score\"].astype(float)\n",
    "        out[\"credit_score_norm\"] = (cs - cs.min()) / (cs.max() - cs.min() + 1e-9)\n",
    "    if {\"loan_to_income\",\"interest_rate\"}.issubset(out.columns):\n",
    "        out[\"lti_x_int\"] = out[\"loan_to_income\"] * out[\"interest_rate\"]\n",
    "    if {\"debt_to_income_ratio\",\"loan_to_income\"}.issubset(out.columns):\n",
    "        out[\"dti_x_lti\"] = out[\"debt_to_income_ratio\"] * out[\"loan_to_income\"]\n",
    "\n",
    "    # Purpose-grade interaction\n",
    "    if {\"grade_letter_ord\",\"loan_purpose\"}.issubset(out.columns):\n",
    "        out[\"purpose_grade\"] = out[\"grade_letter_ord\"] * out[\"loan_purpose\"].astype(str).factorize()[0]\n",
    "\n",
    "    # Categorical hygiene\n",
    "    for c in CAT_COLS_BASE + [\"grade_letter\"]:\n",
    "        if c in out.columns:\n",
    "            out[c] = out[c].astype(str).fillna(\"__MISSING__\")\n",
    "\n",
    "    return out\n",
    "\n",
    "# ------------------------------\n",
    "# WOE / IV functions (fold-safe)\n",
    "# ------------------------------\n",
    "def bin_numeric(x: pd.Series, n_bins: int = 10) -> pd.Series:\n",
    "    # Quantile binning with duplicates handling\n",
    "    try:\n",
    "        return pd.qcut(x, q=n_bins, duplicates=\"drop\")\n",
    "    except Exception:\n",
    "        return pd.cut(x, bins=n_bins, include_lowest=True)\n",
    "\n",
    "def compute_woe_iv(x_bin: pd.Series, y: pd.Series) -> Tuple[pd.Series, float]:\n",
    "    \"\"\"\n",
    "    Compute WOE per bin/category and IV of a variable.\n",
    "    WOE(b) = ln((good_b / total_good) / (bad_b / total_bad))\n",
    "    IV = sum( (good_b/total_good - bad_b/total_bad) * WOE(b) )\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({\"x_bin\": x_bin, \"y\": y})\n",
    "    total_good = (df[\"y\"] == 1).sum()\n",
    "    total_bad = (df[\"y\"] == 0).sum()\n",
    "    # Prevent division by zero\n",
    "    total_good = max(total_good, 1)\n",
    "    total_bad = max(total_bad, 1)\n",
    "\n",
    "    grouped = df.groupby(\"x_bin\")[\"y\"]\n",
    "    good = grouped.sum()\n",
    "    count = grouped.count()\n",
    "    bad = count - good\n",
    "\n",
    "    # smoothing\n",
    "    good_rate = (good + 0.5) / (total_good + 1.0)\n",
    "    bad_rate  = (bad + 0.5) / (total_bad + 1.0)\n",
    "\n",
    "    woe = np.log((good_rate) / (bad_rate))\n",
    "    iv = ((good_rate - bad_rate) * woe).sum()\n",
    "\n",
    "    # Map from bin/category to WOE\n",
    "    woe_map = woe\n",
    "    return woe_map, iv\n",
    "\n",
    "# Safe WOE mapping: always convert bins to a plain string Series before map + fillna\n",
    "def apply_woe_map(x_bin: pd.Series, woe_map: pd.Series, default_woe: float = 0.0) -> pd.Series:\n",
    "    # Convert categories/intervals to string keys, then map to float WOE\n",
    "    x_plain = pd.Series(x_bin.astype(str).values, index=x_bin.index)\n",
    "    mapped = x_plain.map(woe_map)\n",
    "    return mapped.astype(float).fillna(float(default_woe))\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Fold-wise WOE transform builder\n",
    "# ------------------------------\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def build_foldwise_woe(X: pd.DataFrame, y: pd.Series, T: pd.DataFrame,\n",
    "                       num_cols: list, cat_cols: list, n_bins: int = 12):\n",
    "    X_woe = pd.DataFrame(index=X.index)\n",
    "    T_woe = pd.DataFrame(index=T.index)\n",
    "    iv_list = []\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "    # Pre-bin numerics (consistent bin edges), keep as Series of intervals/categories\n",
    "    num_bins_whole = {c: bin_numeric(X[c], n_bins=n_bins) for c in num_cols}\n",
    "    # Categoricals use raw string values\n",
    "    cat_bins_whole = {c: X[c].astype(str) for c in cat_cols}\n",
    "\n",
    "    for c in num_cols + cat_cols:\n",
    "        woe_vals = np.zeros(len(X))  # fold-wise val WOE storage\n",
    "        default_woe = 0.0\n",
    "        iv_vals = []\n",
    "\n",
    "        # Compute WOE map per fold using train split\n",
    "        for tr_idx, val_idx in skf.split(X, y):\n",
    "            x_tr = (num_bins_whole[c] if c in num_cols else cat_bins_whole[c]).iloc[tr_idx]\n",
    "            y_tr = y.iloc[tr_idx]\n",
    "\n",
    "            # WOE/IV on train bins; keys must be strings\n",
    "            woe_map, iv = compute_woe_iv(pd.Series(x_tr.astype(str).values, index=x_tr.index), y_tr)\n",
    "            iv_vals.append(iv)\n",
    "\n",
    "            # Apply to validation bins; cast to string Series before mapping\n",
    "            x_val_bins = (num_bins_whole[c] if c in num_cols else cat_bins_whole[c]).iloc[val_idx]\n",
    "            x_val_bins = pd.Series(x_val_bins.astype(str).values, index=x_val_bins.index)\n",
    "            woe_vals[val_idx] = apply_woe_map(x_val_bins, woe_map, default_woe).values\n",
    "\n",
    "        iv_mean = float(np.mean(iv_vals))\n",
    "        iv_list.append((c + \"_WOE\", iv_mean))\n",
    "\n",
    "        # Build test WOE using full-train WOE; keys must be strings\n",
    "        x_full_bins = (num_bins_whole[c] if c in num_cols else cat_bins_whole[c])\n",
    "        woe_map_full, _ = compute_woe_iv(pd.Series(x_full_bins.astype(str).values, index=x_full_bins.index), y)\n",
    "\n",
    "        # Create test bins for this feature c in-scope (fixes UnboundLocalError)\n",
    "        if c in num_cols:\n",
    "            t_bins = bin_numeric(T[c], n_bins=n_bins)\n",
    "        else:\n",
    "            t_bins = T[c].astype(str)\n",
    "        t_bins = pd.Series(t_bins.astype(str).values, index=T.index)\n",
    "\n",
    "        test_woe = apply_woe_map(t_bins, woe_map_full, default_woe)\n",
    "\n",
    "        X_woe[c + \"_WOE\"] = woe_vals\n",
    "        T_woe[c + \"_WOE\"] = test_woe.values\n",
    "\n",
    "    return X_woe, T_woe, iv_list\n",
    "\n",
    "# ------------------------------\n",
    "# IV-based feature selection\n",
    "# ------------------------------\n",
    "def select_woe_features(iv_list: List[Tuple[str, float]], k_top: int = 40, iv_floor: float = 0.01) -> List[str]:\n",
    "    iv_sorted = sorted(iv_list, key=lambda t: t[1], reverse=True)\n",
    "    candidates = [f for f, iv in iv_sorted if iv >= iv_floor]\n",
    "    return candidates[:k_top]\n",
    "\n",
    "# ------------------------------\n",
    "# Stage-1: Logistic scorecard (base ranking)\n",
    "# ------------------------------\n",
    "def fit_scorecard(X_woe: pd.DataFrame, y: pd.Series, T_woe: pd.DataFrame,\n",
    "                  selected_cols: List[str]) -> Tuple[np.ndarray, np.ndarray, float]:\n",
    "    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "    oof = np.zeros(len(X_woe))\n",
    "    test_pred_accum = np.zeros(len(T_woe))\n",
    "\n",
    "    for tr_idx, val_idx in skf.split(X_woe, y):\n",
    "        X_tr = X_woe.iloc[tr_idx][selected_cols]\n",
    "        X_val = X_woe.iloc[val_idx][selected_cols]\n",
    "        y_tr = y.iloc[tr_idx]\n",
    "        y_val = y.iloc[val_idx]\n",
    "\n",
    "        # Classic L2 logistic\n",
    "        logit = LogisticRegression(\n",
    "            solver=\"lbfgs\", C=2.0, max_iter=2000, random_state=SEED\n",
    "        )\n",
    "        logit.fit(X_tr, y_tr)\n",
    "        oof[val_idx] = logit.predict_proba(X_val)[:, 1]\n",
    "        test_pred_accum += logit.predict_proba(T_woe[selected_cols])[:, 1] / N_FOLDS\n",
    "\n",
    "    auc = roc_auc_score(y, oof)\n",
    "    return oof, test_pred_accum, auc\n",
    "\n",
    "# ------------------------------\n",
    "# Stage-2: Residual LightGBM\n",
    "# ------------------------------\n",
    "def fit_residual_lgb(X_res: pd.DataFrame, y_res: np.ndarray, T_res: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "    oof_res = np.zeros(len(X_res))\n",
    "    test_res_accum = np.zeros(len(T_res))\n",
    "\n",
    "    for tr_idx, val_idx in skf.split(X_res, (y_res > 0.5).astype(int)):\n",
    "        X_tr = X_res.iloc[tr_idx]\n",
    "        X_val = X_res.iloc[val_idx]\n",
    "        y_tr = y_res[tr_idx]\n",
    "        y_val = y_res[val_idx]\n",
    "\n",
    "        model = lgb.LGBMRegressor(**LGB_PARAMS)\n",
    "        model.fit(\n",
    "            X_tr, y_tr,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            eval_metric=\"rmse\",\n",
    "            callbacks=[lgb.early_stopping(stopping_rounds=250, verbose=False)]\n",
    "        )\n",
    "        oof_res[val_idx] = model.predict(X_val)\n",
    "        test_res_accum += model.predict(T_res) / N_FOLDS\n",
    "\n",
    "    return oof_res, test_res_accum\n",
    "\n",
    "# ------------------------------\n",
    "# Pseudo-labeling extremes and refit residual LGB\n",
    "# ------------------------------\n",
    "def pseudo_label_and_refit(X_res: pd.DataFrame, y_res: np.ndarray, T_res: pd.DataFrame, test_pred_res: np.ndarray,\n",
    "                           base_prob_test: np.ndarray, pos_th: float = PL_POS_TH, neg_th: float = PL_NEG_TH,\n",
    "                           max_ratio: float = PL_MAX_RATIO) -> np.ndarray:\n",
    "    # Combine base prob and residual adjustment to get interim test prob\n",
    "    interim_test_prob = np.clip(base_prob_test + test_pred_res, 0.0, 1.0)\n",
    "\n",
    "    # Extreme confident pseudo labels\n",
    "    pos_idx = np.where(interim_test_prob >= pos_th)[0]\n",
    "    neg_idx = np.where(interim_test_prob <= neg_th)[0]\n",
    "    pl_idx = np.concatenate([pos_idx, neg_idx])\n",
    "\n",
    "    # Cap pseudo label count\n",
    "    cap = int(max_ratio * len(X_res))\n",
    "    if len(pl_idx) > cap:\n",
    "        pl_idx = pl_idx[:cap]\n",
    "\n",
    "    if len(pl_idx) == 0:\n",
    "        # No pseudo labels → return original residual predictions\n",
    "        return test_pred_res\n",
    "\n",
    "    # Build pseudo-labeled dataset\n",
    "    X_pl = T_res.iloc[pl_idx]\n",
    "    y_pl = np.where(interim_test_prob[pl_idx] >= pos_th, 1.0, 0.0)\n",
    "\n",
    "    X_aug = pd.concat([X_res, X_pl], axis=0, ignore_index=True)\n",
    "    y_aug = np.concatenate([y_res, y_pl], axis=0)\n",
    "\n",
    "    # Refit single residual LGB on augmented data (no CV for speed)\n",
    "    model = lgb.LGBMRegressor(**LGB_PARAMS)\n",
    "    model.fit(X_aug, y_aug)\n",
    "    test_pred_refit = model.predict(T_res)\n",
    "    return test_pred_refit\n",
    "\n",
    "# ------------------------------\n",
    "# Main\n",
    "# ------------------------------\n",
    "def main():\n",
    "    X_raw, y, T_raw, test_ids = load_data()\n",
    "    X = engineer(X_raw)\n",
    "    T = engineer(T_raw)\n",
    "\n",
    "    # Define full feature sets for WOE\n",
    "    num_cols = [c for c in [\n",
    "        \"annual_income\",\"debt_to_income_ratio\",\"credit_score\",\n",
    "        \"loan_amount\",\"interest_rate\",\"loan_to_income\",\n",
    "        \"log_loan_to_income\",\"interest_burden\",\"credit_score_norm\",\n",
    "        \"lti_x_int\",\"dti_x_lti\"\n",
    "    ] if c in X.columns]\n",
    "\n",
    "    cat_cols = [c for c in [\n",
    "        \"gender\",\"marital_status\",\"education_level\",\n",
    "        \"employment_status\",\"loan_purpose\",\n",
    "        \"grade_letter\",\"subgrade_num\"  # parsed from grade_subgrade\n",
    "    ] if c in X.columns]\n",
    "\n",
    "    # Build fold-wise WOE transforms\n",
    "    X_woe, T_woe, iv_list = build_foldwise_woe(X, y, T, num_cols=num_cols, cat_cols=cat_cols, n_bins=12)\n",
    "\n",
    "    # Select top WOE features by IV (keep strong signals, reduce noise)\n",
    "    selected_woe_cols = select_woe_features(iv_list, k_top=50, iv_floor=0.01)\n",
    "\n",
    "    # Stage-1: Base scorecard\n",
    "    oof_base, test_base, auc_base = fit_scorecard(X_woe, y, T_woe, selected_woe_cols)\n",
    "    print(f\"Stage-1 Scorecard OOF AUC: {auc_base:.5f}\")\n",
    "\n",
    "    # Residual target (clipped)\n",
    "    residual_oof_target = np.clip(y.values - oof_base, -1.0, 1.0)\n",
    "    residual_test_target = np.zeros(len(T))  # placeholder, not used directly\n",
    "\n",
    "    # Residual features: combine WOE + original engineered numerics (no raw categoricals)\n",
    "    res_cols = selected_woe_cols + [c for c in num_cols if c in X.columns]\n",
    "    X_res = pd.DataFrame(np.c_[X_woe[selected_woe_cols].values, X[ [c for c in num_cols if c in X.columns] ].values],\n",
    "                         index=X.index)\n",
    "    T_res = pd.DataFrame(np.c_[T_woe[selected_woe_cols].values, T[ [c for c in num_cols if c in T.columns] ].values],\n",
    "                         index=T.index)\n",
    "\n",
    "    # Stage-2: Residual LGB (CV)\n",
    "    oof_residual, test_residual = fit_residual_lgb(X_res, residual_oof_target, T_res)\n",
    "\n",
    "    # Pseudo-label extreme test cases and refit residual model for improved generalization\n",
    "    test_residual_refit = pseudo_label_and_refit(X_res, residual_oof_target, T_res, test_residual, test_base,\n",
    "                                                 pos_th=PL_POS_TH, neg_th=PL_NEG_TH, max_ratio=PL_MAX_RATIO)\n",
    "\n",
    "    # Combine base + residual\n",
    "    oof_comb = np.clip(oof_base + oof_residual, 0.0, 1.0)\n",
    "    test_comb = np.clip(test_base + test_residual_refit, 0.0, 1.0)\n",
    "\n",
    "    # Final isotonic calibration on OOF\n",
    "    iso = IsotonicRegression(out_of_bounds=\"clip\")\n",
    "    iso.fit(oof_comb, y.values)\n",
    "    test_prob = np.clip(iso.predict(test_comb), 0.0, 1.0)\n",
    "\n",
    "    # Submission\n",
    "    pd.DataFrame({ID_COL: test_ids, TARGET: test_prob}).to_csv(\"submission.csv\", index=False)\n",
    "    print(\"\\nSaved submission.csv\")\n",
    "\n",
    "    # Report\n",
    "    print(f\"Final OOF AUC (post residual): {roc_auc_score(y, oof_comb):.5f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 14262372,
     "sourceId": 91722,
     "sourceType": "competition"
    },
    {
     "datasetId": 902,
     "sourceId": 370089,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8678604,
     "sourceId": 13651350,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8678647,
     "sourceId": 13651405,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 454.852739,
   "end_time": "2025-11-09T23:05:25.517782",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-09T22:57:50.665043",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
