{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91722,"databundleVersionId":14262372,"isSourceIdPinned":false,"sourceType":"competition"}],"dockerImageVersionId":31153,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# filename: main.py\n# Kaggle S5E11 â€” Loan Payback\n# Robust CatBoost + LightGBM ensemble with per-fold target encoding, category dtype for LGB,\n# monotone constraints on trusted features, and OOF blend weight search.\n\nimport os\nimport warnings\nfrom typing import List, Tuple\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.isotonic import IsotonicRegression\n\nfrom catboost import CatBoostClassifier, Pool\nimport lightgbm as lgb\n\nwarnings.filterwarnings(\"ignore\")\nSEED, N_FOLDS = 42, 5\n\n# ------------------------------\n# Data\n# ------------------------------\ndef load_data(target_col=\"loan_paid_back\", id_col=\"id\"):\n    train_path = \"train.csv\" if os.path.exists(\"train.csv\") else \"/kaggle/input/playground-series-s5e11/train.csv\"\n    test_path  = \"test.csv\"  if os.path.exists(\"test.csv\")  else \"/kaggle/input/playground-series-s5e11/test.csv\"\n    train, test = pd.read_csv(train_path), pd.read_csv(test_path)\n    y = train[target_col].astype(float)\n    X = train.drop(columns=[target_col])\n    test_ids = test[id_col].copy()\n    return X, y, test, test_ids\n\n# ------------------------------\n# Features\n# ------------------------------\ndef engineer(df: pd.DataFrame) -> pd.DataFrame:\n    out = df.copy()\n    for c in [\"annual_income\",\"debt_to_income_ratio\",\"credit_score\",\"loan_amount\",\"interest_rate\"]:\n        if c in out.columns: out[c] = pd.to_numeric(out[c], errors=\"coerce\")\n\n    if {\"loan_amount\",\"annual_income\"}.issubset(out.columns):\n        out[\"loan_to_income\"] = out[\"loan_amount\"]/(out[\"annual_income\"]+1.0)\n        out[\"log_loan_to_income\"] = np.log1p(out[\"loan_to_income\"].clip(lower=0))\n\n    if {\"interest_rate\",\"debt_to_income_ratio\"}.issubset(out.columns):\n        out[\"interest_burden\"] = out[\"interest_rate\"]*out[\"debt_to_income_ratio\"]\n\n    if \"credit_score\" in out.columns:\n        cs = out[\"credit_score\"].astype(float)\n        out[\"credit_score_norm\"] = (cs - cs.min())/(cs.max() - cs.min() + 1e-9)\n\n    if \"grade_subgrade\" in out.columns:\n        g = out[\"grade_subgrade\"].astype(str)\n        out[\"grade_letter\"] = g.str[0]\n        letter_map = {\"A\":6,\"B\":5,\"C\":4,\"D\":3,\"E\":2,\"F\":1}\n        out[\"grade_letter_ord\"] = out[\"grade_letter\"].map(letter_map).fillna(3).astype(int)\n\n    if {\"loan_to_income\",\"interest_rate\"}.issubset(out.columns):\n        out[\"lti_x_int\"] = out[\"loan_to_income\"]*out[\"interest_rate\"]\n    if {\"debt_to_income_ratio\",\"loan_to_income\"}.issubset(out.columns):\n        out[\"dti_x_lti\"] = out[\"debt_to_income_ratio\"]*out[\"loan_to_income\"]\n    return out\n\ndef infer_cats(df: pd.DataFrame, max_card=64) -> List[str]:\n    cats=[]\n    for c in df.columns:\n        if df[c].dtype == \"object\":\n            cats.append(c)\n        elif pd.api.types.is_integer_dtype(df[c]) and df[c].nunique() <= max_card:\n            cats.append(c)\n    return cats\n\n# ------------------------------\n# Strict OOF target mean encoding\n# ------------------------------\ndef target_mean_encode_cv(X: pd.DataFrame, y: pd.Series, T: pd.DataFrame, cols: List[str]) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    X_enc, T_enc = X.copy(), T.copy()\n    global_mean = y.mean()\n    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n    for col in cols:\n        oof = np.zeros(len(X))\n        full_map = y.groupby(X[col]).mean()\n        T_map = T[col].map(full_map).fillna(global_mean)\n        for tr_idx, val_idx in skf.split(X, y):\n            fold_map = y.iloc[tr_idx].groupby(X.iloc[tr_idx][col]).mean()\n            oof[val_idx] = X.iloc[val_idx][col].map(fold_map).fillna(global_mean).values\n        X_enc[f\"{col}_tgtmean\"] = oof\n        T_enc[f\"{col}_tgtmean\"] = T_map.values\n    return X_enc, T_enc\n\n# ------------------------------\n# Models\n# ------------------------------\nCAT_PARAMS = {\n    \"loss_function\":\"Logloss\",\n    \"eval_metric\":\"AUC\",\n    \"learning_rate\":0.033,\n    \"depth\":7,\n    \"l2_leaf_reg\":5.0,\n    \"iterations\":3200,\n    \"bootstrap_type\":\"Bayesian\",\n    \"random_seed\":SEED,\n    \"verbose\":False,\n    \"od_type\":\"Iter\",\n    \"od_wait\":500,\n}\n\nLGB_PARAMS = {\n    \"objective\":\"binary\",\n    \"metric\":\"auc\",\n    \"learning_rate\":0.03,\n    \"num_leaves\":56,\n    \"feature_fraction\":0.9,\n    \"bagging_fraction\":0.9,\n    \"bagging_freq\":1,\n    \"min_data_in_leaf\":50,\n    \"lambda_l2\":6.0,\n    \"lambda_l1\":0.0,\n    \"max_depth\":-1,\n    \"n_estimators\":2800,\n    \"random_state\":SEED,\n    \"verbosity\":-1,\n}\n\ndef build_monotone_constraints(columns: List[str]) -> List[int]:\n    pos = {\"interest_rate\",\"debt_to_income_ratio\",\"loan_to_income\"}\n    neg = {\"credit_score_norm\"}\n    return [1 if c in pos else (-1 if c in neg else 0) for c in columns]\n\n# ------------------------------\n# Training\n# ------------------------------\ndef run():\n    X_raw, y, T_raw, test_ids = load_data()\n    X, T = engineer(X_raw), engineer(T_raw)\n    common = [c for c in X.columns if c in T.columns]\n    X, T = X[common].copy(), T[common].copy()\n\n    # Categorical handling\n    cat_cols = infer_cats(X)\n    for c in cat_cols:\n        X[c] = X[c].astype(\"object\").fillna(\"__MISSING__\")\n        T[c] = T[c].astype(\"object\").fillna(\"__MISSING__\")\n\n    # Target mean encoding on strongest cats\n    high_signal = [c for c in [\"grade_subgrade\",\"loan_purpose\",\"employment_status\"] if c in cat_cols]\n    X_te, T_te = target_mean_encode_cv(X, y, T, high_signal)\n\n    # LightGBM: use pandas category dtype on original object columns\n    X_lgb = X_te.copy()\n    T_lgb = T_te.copy()\n    for c in cat_cols:\n        if X_lgb[c].dtype == \"object\":\n            X_lgb[c] = X_lgb[c].astype(\"category\")\n            T_lgb[c] = T_lgb[c].astype(\"category\")\n\n    # CatBoost categorical indices (on the TE frame which includes raw cats)\n    cb_cat_idx = [X_te.columns.get_loc(c) for c in cat_cols if c in X_te.columns]\n    mono = build_monotone_constraints(list(X_lgb.columns))\n\n    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n    oof_cb = np.zeros(len(X))\n    oof_lgb = np.zeros(len(X))\n\n    cb_models, lgb_models = [], []\n\n    for fold, (tr_idx, val_idx) in enumerate(skf.split(X, y), start=1):\n        print(f\"Fold {fold}/{N_FOLDS}\")\n\n        cb = CatBoostClassifier(**CAT_PARAMS)\n        cb.fit(\n            Pool(X_te.iloc[tr_idx], label=y.iloc[tr_idx], cat_features=cb_cat_idx if cb_cat_idx else None),\n            eval_set=Pool(X_te.iloc[val_idx], label=y.iloc[val_idx], cat_features=cb_cat_idx if cb_cat_idx else None),\n            use_best_model=True,\n            verbose=False\n        )\n        preds_cb = cb.predict_proba(Pool(X_te.iloc[val_idx], cat_features=cb_cat_idx if cb_cat_idx else None))[:,1]\n        oof_cb[val_idx] = preds_cb\n        cb_models.append(cb)\n        print(f\"  CatBoost AUC: {roc_auc_score(y.iloc[val_idx], preds_cb):.5f}\")\n\n        lgbm = lgb.LGBMClassifier(**LGB_PARAMS, monotone_constraints=mono)\n        lgbm.fit(\n            X_lgb.iloc[tr_idx], y.iloc[tr_idx],\n            eval_set=[(X_lgb.iloc[val_idx], y.iloc[val_idx])],\n            eval_metric=\"auc\",\n            callbacks=[lgb.early_stopping(stopping_rounds=250, verbose=False)]\n        )\n        preds_lgb = lgbm.predict_proba(X_lgb.iloc[val_idx])[:,1]\n        oof_lgb[val_idx] = preds_lgb\n        lgb_models.append(lgbm)\n        print(f\"  LightGBM AUC: {roc_auc_score(y.iloc[val_idx], preds_lgb):.5f}\")\n\n    # OOF weight search for blend\n    weights = np.linspace(0.5, 0.8, 7)  # CatBoost weight candidates\n    best_w, best_auc = None, -1.0\n    for w in weights:\n        oof_blend = w*oof_cb + (1.0-w)*oof_lgb\n        auc_blend = roc_auc_score(y, oof_blend)\n        if auc_blend > best_auc:\n            best_auc, best_w = auc_blend, w\n    print(f\"\\nBest OOF blend: weight_cb={best_w:.3f}, AUC={best_auc:.5f}\")\n\n    # Calibrate best OOF blend\n    oof_blend_best = best_w*oof_cb + (1.0-best_w)*oof_lgb\n    iso = IsotonicRegression(out_of_bounds=\"clip\").fit(oof_blend_best, y.values)\n\n    # Full fits\n    cb_full = CatBoostClassifier(**CAT_PARAMS)\n    cb_full.fit(Pool(X_te, label=y, cat_features=cb_cat_idx if cb_cat_idx else None), verbose=False)\n\n    lgb_full = lgb.LGBMClassifier(**LGB_PARAMS, monotone_constraints=mono)\n    lgb_full.fit(X_lgb, y)\n\n    # Test predictions and final calibrated blend\n    test_cb = cb_full.predict_proba(Pool(T_te, cat_features=cb_cat_idx if cb_cat_idx else None))[:,1]\n    test_lgb = lgb_full.predict_proba(T_lgb)[:,1]\n    test_blend = best_w*test_cb + (1.0-best_w)*test_lgb\n    test_blend_cal = np.clip(iso.predict(test_blend), 0.0, 1.0)\n\n    pd.DataFrame({\"id\": test_ids, \"loan_paid_back\": test_blend_cal}).to_csv(\"submission.csv\", index=False)\n    print(\"\\nSaved submission.csv\")\n\nif __name__ == \"__main__\":\n    run()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}