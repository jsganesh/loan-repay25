{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5231180",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-11T03:34:28.243380Z",
     "iopub.status.busy": "2025-11-11T03:34:28.242909Z",
     "iopub.status.idle": "2025-11-11T05:58:11.029369Z",
     "shell.execute_reply": "2025-11-11T05:58:11.028276Z"
    },
    "papermill": {
     "duration": 8622.7923,
     "end_time": "2025-11-11T05:58:11.031003",
     "exception": false,
     "start_time": "2025-11-11T03:34:28.238703",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "KAGGLE LOAN PAYBACK - LGB-FOCUSED OPTIMIZED PIPELINE\n",
      "============================================================\n",
      "\n",
      "Loading data...\n",
      "Train shape: (593994, 12), Test shape: (254569, 12)\n",
      "\n",
      "Engineering features...\n",
      "After engineering - Train: (593994, 27), Test: (254569, 27)\n",
      "\n",
      "Applying fold-wise encodings...\n",
      "  ✓ Encoded: grade_letter\n",
      "  ✓ Encoded: subgrade_num\n",
      "  ✓ Encoded: loan_purpose\n",
      "  ✓ Encoded: employment_status\n",
      "  ✓ Encoded: education_level\n",
      "  ✓ Encoded: marital_status\n",
      "\n",
      "Label-encoding categorical columns...\n",
      "\n",
      "Applying variance filtering...\n",
      "Dropped low-variance features: ['subgrade_num_te_smooth', 'loan_purpose_te_smooth', 'education_level_te_smooth', 'marital_status_te_smooth']\n",
      "Categorical columns: ['gender', 'marital_status', 'education_level', 'employment_status', 'loan_purpose', 'grade_subgrade', 'grade_letter', 'subgrade_num', 'grade_letter_ord', 'purpose_grade', 'income_decile']\n",
      "\n",
      "============================================================\n",
      "TRAINING LGB ENSEMBLE (5-fold CV, 15 models/fold)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Fold 1/5\n",
      "============================================================\n",
      "  ✓ LGB OOF AUC: 0.92334\n",
      "\n",
      "============================================================\n",
      "Fold 2/5\n",
      "============================================================\n",
      "  ✓ LGB OOF AUC: 0.92274\n",
      "\n",
      "============================================================\n",
      "Fold 3/5\n",
      "============================================================\n",
      "  ✓ LGB OOF AUC: 0.92131\n",
      "\n",
      "============================================================\n",
      "Fold 4/5\n",
      "============================================================\n",
      "  ✓ LGB OOF AUC: 0.92224\n",
      "\n",
      "============================================================\n",
      "Fold 5/5\n",
      "============================================================\n",
      "  ✓ LGB OOF AUC: 0.92174\n",
      "\n",
      "============================================================\n",
      "Fitting final Platt calibration...\n",
      "============================================================\n",
      "  ✓ Final OOF AUC: 0.92227\n",
      "\n",
      "============================================================\n",
      "SAVING SUBMISSION\n",
      "============================================================\n",
      "✓ Saved submission.csv\n",
      "  Shape: (254569, 2)\n",
      "  Predicted prob range: [0.0235, 0.9634]\n"
     ]
    }
   ],
   "source": [
    "# filename: back_to_lgb_.py\n",
    "# Kaggle S5E11 — Loan Payback\n",
    "# Strategy: LGB-focused ensemble (LGB is best at 0.9235, CB/XGB are weak)\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# ==============================\n",
    "# CONFIG\n",
    "# ==============================\n",
    "SEED = 42\n",
    "N_FOLDS = 5\n",
    "TARGET_COL = \"loan_paid_back\"\n",
    "ID_COL = \"id\"\n",
    "\n",
    "# ===== MAXIMIZE LGB (the only consistently strong model) =====\n",
    "SEEDS_LGB = [42, 2025, 777, 131, 999]          # INCREASED: 3→5 seeds for more diversity\n",
    "SEEDS_CB  = []                                  # SKIP: too slow, mediocre (0.9197)\n",
    "SEEDS_XGB = []                                  # SKIP: too slow, weakest (0.9176)\n",
    "\n",
    "LR_SWEEP_LGB = [0.028, 0.030, 0.032]          # Sweep LR for LGB diversity\n",
    "\n",
    "# ===== OPTIMIZED LGB PARAMS (less aggressive regularization) =====\n",
    "BASE_LGB_PARAMS = {\n",
    "    \"objective\": \"binary\", \"metric\": \"auc\",\n",
    "    \"learning_rate\": 0.030, \"num_leaves\": 80,  # was 64 → stronger trees\n",
    "    \"feature_fraction\": 0.90, \"bagging_fraction\": 0.90, \"bagging_freq\": 1,  # less aggressive\n",
    "    \"min_data_in_leaf\": 50,  # was 70 → allow more splits\n",
    "    \"lambda_l2\": 8.0,        # was 12.0 → less regularization\n",
    "    \"lambda_l1\": 0.2,        # was 0.5 → lighter L1\n",
    "    \"max_depth\": -1,         # was 6 → no depth constraint (LGB is good at this)\n",
    "    \"n_estimators\": 2000,    # was 1600 → more iterations\n",
    "    \"verbosity\": -1,\n",
    "}\n",
    "\n",
    "# ==============================\n",
    "# IO\n",
    "# ==============================\n",
    "def load_data() -> Tuple[pd.DataFrame, pd.Series, pd.DataFrame, pd.Series]:\n",
    "    \"\"\"Load train/test data\"\"\"\n",
    "    train_path = \"train.csv\" if os.path.exists(\"train.csv\") else \"/kaggle/input/playground-series-s5e11/train.csv\"\n",
    "    test_path  = \"test.csv\"  if os.path.exists(\"test.csv\")  else \"/kaggle/input/playground-series-s5e11/test.csv\"\n",
    "    train = pd.read_csv(train_path)\n",
    "    test = pd.read_csv(test_path)\n",
    "    y = train[TARGET_COL].astype(float)\n",
    "    X = train.drop(columns=[TARGET_COL])\n",
    "    test_ids = test[ID_COL].copy()\n",
    "    return X, y, test, test_ids\n",
    "\n",
    "# ==============================\n",
    "# FEATURE ENGINEERING\n",
    "# ==============================\n",
    "NUM_BASE = [\"annual_income\", \"debt_to_income_ratio\", \"credit_score\", \"loan_amount\", \"interest_rate\"]\n",
    "CAT_BASE = [\"gender\", \"marital_status\", \"education_level\", \"employment_status\", \"loan_purpose\", \"grade_subgrade\"]\n",
    "\n",
    "def engineer(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Enhanced feature engineering\"\"\"\n",
    "    out = df.copy()\n",
    "\n",
    "    for c in NUM_BASE:\n",
    "        if c in out.columns:\n",
    "            out[c] = pd.to_numeric(out[c], errors=\"coerce\")\n",
    "\n",
    "    if \"grade_subgrade\" in out.columns:\n",
    "        g = out[\"grade_subgrade\"].astype(str)\n",
    "        out[\"grade_letter\"] = g.str[0]\n",
    "        out[\"subgrade_num\"] = pd.to_numeric(g.str[1:], errors=\"coerce\").fillna(3)\n",
    "        letter_map = {\"A\": 6, \"B\": 5, \"C\": 4, \"D\": 3, \"E\": 2, \"F\": 1}\n",
    "        out[\"grade_letter_ord\"] = out[\"grade_letter\"].map(letter_map).fillna(3).astype(int)\n",
    "\n",
    "    if {\"loan_amount\", \"annual_income\"}.issubset(out.columns):\n",
    "        out[\"loan_to_income\"] = out[\"loan_amount\"] / (out[\"annual_income\"] + 1.0)\n",
    "        out[\"log_loan_to_income\"] = np.log1p(out[\"loan_to_income\"].clip(lower=0))\n",
    "    if {\"interest_rate\", \"debt_to_income_ratio\"}.issubset(out.columns):\n",
    "        out[\"interest_burden\"] = out[\"interest_rate\"] * out[\"debt_to_income_ratio\"]\n",
    "    if \"credit_score\" in out.columns:\n",
    "        cs = out[\"credit_score\"].astype(float)\n",
    "        out[\"credit_score_norm\"] = (cs - cs.min()) / (cs.max() - cs.min() + 1e-9)\n",
    "    if {\"loan_to_income\", \"interest_rate\"}.issubset(out.columns):\n",
    "        out[\"lti_x_int\"] = out[\"loan_to_income\"] * out[\"interest_rate\"]\n",
    "    if {\"debt_to_income_ratio\", \"loan_to_income\"}.issubset(out.columns):\n",
    "        out[\"dti_x_lti\"] = out[\"debt_to_income_ratio\"] * out[\"loan_to_income\"]\n",
    "    if {\"grade_letter_ord\", \"loan_purpose\"}.issubset(out.columns):\n",
    "        out[\"purpose_grade\"] = out[\"grade_letter_ord\"] * out[\"loan_purpose\"].astype(str).factorize()[0]\n",
    "\n",
    "    if {\"interest_rate\", \"credit_score\"}.issubset(out.columns):\n",
    "        cs = out[\"credit_score\"].astype(float)\n",
    "        ir = out[\"interest_rate\"].astype(float)\n",
    "        out[\"ir_over_credit\"] = ir / (cs / 100.0 + 1.0)\n",
    "\n",
    "    if {\"loan_amount\", \"debt_to_income_ratio\"}.issubset(out.columns):\n",
    "        out[\"loan_x_dti\"] = out[\"loan_amount\"] * out[\"debt_to_income_ratio\"]\n",
    "\n",
    "    if \"annual_income\" in out.columns:\n",
    "        inc = out[\"annual_income\"].astype(float)\n",
    "        out[\"log_income\"] = np.log1p(inc.clip(lower=0))\n",
    "        out[\"income_decile\"] = pd.qcut(inc, q=10, labels=False, duplicates=\"drop\")\n",
    "\n",
    "    if \"credit_score\" in out.columns:\n",
    "        cs_norm = (out[\"credit_score\"].astype(float) - 300.0) / (850.0 - 300.0)\n",
    "        cs_norm = cs_norm.clip(0, 1)\n",
    "        out[\"credit_score_sq\"] = cs_norm ** 2\n",
    "\n",
    "    for c in CAT_BASE + [\"grade_letter\"]:\n",
    "        if c in out.columns:\n",
    "            out[c] = out[c].astype(str).fillna(\"__MISSING__\")\n",
    "\n",
    "    return out\n",
    "\n",
    "def infer_cats(df: pd.DataFrame, max_card: int = 64) -> List[str]:\n",
    "    cats = []\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype == \"object\":\n",
    "            cats.append(c)\n",
    "        elif pd.api.types.is_integer_dtype(df[c]) and df[c].nunique() <= max_card:\n",
    "            cats.append(c)\n",
    "    return cats\n",
    "\n",
    "# ==============================\n",
    "# VARIANCE FILTER\n",
    "# ==============================\n",
    "def simple_variance_filter(X: pd.DataFrame, threshold: float = 0.001) -> pd.DataFrame:\n",
    "    numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    dropped = []\n",
    "    for col in numeric_cols:\n",
    "        if X[col].var() < threshold:\n",
    "            X = X.drop(columns=[col])\n",
    "            dropped.append(col)\n",
    "    if dropped:\n",
    "        print(f\"Dropped low-variance features: {dropped}\")\n",
    "    return X\n",
    "\n",
    "# ==============================\n",
    "# FOLD-WISE ENCODINGS\n",
    "# ==============================\n",
    "def fold_smoothed_target_encode(X: pd.DataFrame, y: pd.Series, T: pd.DataFrame, col: str,\n",
    "                                n_splits: int = N_FOLDS, prior: int = 10, seed: int = SEED) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    X_col = X[col].astype(str)\n",
    "    T_col = T[col].astype(str)\n",
    "    X_enc = np.zeros(len(X))\n",
    "    global_mean = y.mean()\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    \n",
    "    for tr_idx, val_idx in skf.split(X, y):\n",
    "        te_map = y.iloc[tr_idx].groupby(X_col.iloc[tr_idx]).agg(['mean', 'count'])\n",
    "        smooth = (te_map['mean'] * te_map['count'] + global_mean * prior) / (te_map['count'] + prior)\n",
    "        mapping = smooth.to_dict()\n",
    "        X_enc[val_idx] = X_col.iloc[val_idx].map(mapping).fillna(global_mean).values\n",
    "    \n",
    "    full_map = y.groupby(X_col).agg(['mean', 'count'])\n",
    "    full_smooth = (full_map['mean'] * full_map['count'] + global_mean * prior) / (full_map['count'] + prior)\n",
    "    T_enc = T_col.map(full_smooth.to_dict()).fillna(global_mean).values\n",
    "    return X_enc, T_enc\n",
    "\n",
    "def fold_frequency_encode(X: pd.DataFrame, y: pd.Series, T: pd.DataFrame, col: str,\n",
    "                          n_splits: int = N_FOLDS, seed: int = SEED) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    X_col = X[col].astype(str)\n",
    "    T_col = T[col].astype(str)\n",
    "    X_freq = np.zeros(len(X))\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    \n",
    "    for tr_idx, val_idx in skf.split(X, y):\n",
    "        freq = X_col.iloc[tr_idx].value_counts().to_dict()\n",
    "        X_freq[val_idx] = X_col.iloc[val_idx].map(freq).fillna(0).values\n",
    "    \n",
    "    full_freq = X_col.value_counts().to_dict()\n",
    "    T_freq = T_col.map(full_freq).fillna(0).values\n",
    "    return X_freq, T_freq\n",
    "\n",
    "# ==============================\n",
    "# UTILITY FUNCTIONS\n",
    "# ==============================\n",
    "def to_rank(a: np.ndarray) -> np.ndarray:\n",
    "    order = a.argsort()\n",
    "    ranks = np.empty_like(order, dtype=float)\n",
    "    ranks[order] = np.linspace(0.0, 1.0, len(a), endpoint=True)\n",
    "    return ranks\n",
    "\n",
    "def fit_platt_calibration(probs: np.ndarray, targets: np.ndarray) -> LogisticRegression:\n",
    "    lr = LogisticRegression(max_iter=500, random_state=SEED, solver=\"lbfgs\")\n",
    "    lr.fit(probs.reshape(-1, 1), targets)\n",
    "    return lr\n",
    "\n",
    "def apply_platt_calibration(lr: LogisticRegression, probs: np.ndarray) -> np.ndarray:\n",
    "    return np.clip(lr.predict_proba(probs.reshape(-1, 1))[:, 1], 0.0, 1.0)\n",
    "\n",
    "# ==============================\n",
    "# LGB-ONLY OOF FEATURES\n",
    "# ==============================\n",
    "def get_oof_features_lgb_only(X: pd.DataFrame, y: pd.Series, T: pd.DataFrame,\n",
    "                              cat_cols: List[str]) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Generate LGB OOF predictions with 5 diverse seeds + 3 LR values\"\"\"\n",
    "    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "    oof_probs = np.zeros(len(X))\n",
    "    test_preds_all = []\n",
    "\n",
    "    for fold, (tr_idx, val_idx) in enumerate(skf.split(X, y), start=1):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Fold {fold}/{N_FOLDS}\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        val_probs = []\n",
    "        test_preds = []\n",
    "\n",
    "        # 5 seeds × 3 LRs = 15 LGB models per fold\n",
    "        for seed_idx, s in enumerate(SEEDS_LGB):\n",
    "            for lr_idx, lr in enumerate(LR_SWEEP_LGB):\n",
    "                params_lgb = BASE_LGB_PARAMS.copy()\n",
    "                params_lgb[\"random_state\"] = s\n",
    "                params_lgb[\"learning_rate\"] = lr\n",
    "                \n",
    "                # Jitter for diversity\n",
    "                jitter_sign = 1 if (seed_idx + lr_idx) % 2 == 0 else -1\n",
    "                params_lgb[\"feature_fraction\"] = params_lgb[\"feature_fraction\"] + jitter_sign * 0.02\n",
    "                params_lgb[\"bagging_fraction\"] = params_lgb[\"bagging_fraction\"] + jitter_sign * 0.02\n",
    "\n",
    "                model = lgb.LGBMClassifier(**params_lgb)\n",
    "                model.fit(\n",
    "                    X.iloc[tr_idx], y.iloc[tr_idx],\n",
    "                    eval_set=[(X.iloc[val_idx], y.iloc[val_idx])],\n",
    "                    eval_metric=\"auc\",\n",
    "                    callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)]\n",
    "                )\n",
    "                vp = model.predict_proba(X.iloc[val_idx])[:, 1]\n",
    "                val_probs.append(vp)\n",
    "                test_preds.append(model.predict_proba(T)[:, 1])\n",
    "\n",
    "        oof_probs[val_idx] = np.mean(val_probs, axis=0)\n",
    "        test_preds_all.append(np.mean(np.vstack(test_preds), axis=0))\n",
    "        \n",
    "        auc = roc_auc_score(y.iloc[val_idx], oof_probs[val_idx])\n",
    "        print(f\"  ✓ LGB OOF AUC: {auc:.5f}\")\n",
    "\n",
    "    test_preds_final = np.mean(np.vstack(test_preds_all), axis=0)\n",
    "    return oof_probs, test_preds_final\n",
    "\n",
    "# ==============================\n",
    "# MAIN PIPELINE\n",
    "# ==============================\n",
    "def main():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"KAGGLE LOAN PAYBACK - LGB-FOCUSED OPTIMIZED PIPELINE\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "    # ===== LOAD & ENGINEER =====\n",
    "    print(\"Loading data...\")\n",
    "    X_raw, y, T_raw, test_ids = load_data()\n",
    "    print(f\"Train shape: {X_raw.shape}, Test shape: {T_raw.shape}\")\n",
    "\n",
    "    print(\"\\nEngineering features...\")\n",
    "    X = engineer(X_raw)\n",
    "    T = engineer(T_raw)\n",
    "    print(f\"After engineering - Train: {X.shape}, Test: {T.shape}\")\n",
    "\n",
    "    # ===== FOLD-WISE ENCODINGS =====\n",
    "    print(\"\\nApplying fold-wise encodings...\")\n",
    "    strong_cats = [\n",
    "        c for c in [\"grade_letter\", \"subgrade_num\", \"loan_purpose\", \n",
    "                    \"employment_status\", \"education_level\", \"marital_status\"]\n",
    "        if c in X.columns\n",
    "    ]\n",
    "    \n",
    "    X_enc, T_enc = X.copy(), T.copy()\n",
    "    for col in strong_cats:\n",
    "        prior = 20 if col in {\"employment_status\", \"education_level\"} else 10\n",
    "        \n",
    "        x_te, t_te = fold_smoothed_target_encode(X, y, T, col, n_splits=N_FOLDS, prior=prior, seed=SEED)\n",
    "        x_fe, t_fe = fold_frequency_encode(X, y, T, col, n_splits=N_FOLDS, seed=SEED)\n",
    "        \n",
    "        X_enc[f\"{col}_te_smooth\"] = x_te\n",
    "        T_enc[f\"{col}_te_smooth\"] = t_te\n",
    "        X_enc[f\"{col}_freq\"] = x_fe\n",
    "        T_enc[f\"{col}_freq\"] = t_fe\n",
    "        print(f\"  ✓ Encoded: {col}\")\n",
    "\n",
    "    # ===== LABEL-ENCODE =====\n",
    "    print(\"\\nLabel-encoding categorical columns...\")\n",
    "    for c in X_enc.columns:\n",
    "        if X_enc[c].dtype == \"object\":\n",
    "            combined = pd.concat([X_enc[c].astype(str), T_enc[c].astype(str)], axis=0)\n",
    "            codes = pd.Categorical(combined).codes\n",
    "            X_enc[c] = codes[:len(X_enc)]\n",
    "            T_enc[c] = codes[len(X_enc):]\n",
    "\n",
    "    # ===== VARIANCE FILTER =====\n",
    "    print(\"\\nApplying variance filtering...\")\n",
    "    X_clean = simple_variance_filter(X_enc.copy(), threshold=0.001)\n",
    "    T_clean = T_enc[[c for c in T_enc.columns if c in X_clean.columns]]\n",
    "\n",
    "    # ===== INFER CATS =====\n",
    "    cat_cols = infer_cats(X_clean)\n",
    "    print(f\"Categorical columns: {cat_cols}\")\n",
    "    \n",
    "    for c in cat_cols:\n",
    "        if X_clean[c].dtype == \"object\":\n",
    "            X_clean[c] = X_clean[c].astype(\"category\")\n",
    "            T_clean[c] = T_clean[c].astype(\"category\")\n",
    "\n",
    "    # ===== LGB-ONLY TRAINING =====\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TRAINING LGB ENSEMBLE (5-fold CV, 15 models/fold)\")\n",
    "    print(\"=\"*60)\n",
    "    oof_probs, test_preds = get_oof_features_lgb_only(X_clean, y, T_clean, cat_cols=cat_cols)\n",
    "\n",
    "    # ===== FINAL CALIBRATION =====\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Fitting final Platt calibration...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    lr_final = fit_platt_calibration(oof_probs, y.values)\n",
    "    test_prob = apply_platt_calibration(lr_final, test_preds)\n",
    "\n",
    "    oof_calib = apply_platt_calibration(lr_final, oof_probs)\n",
    "    auc_final = roc_auc_score(y, oof_calib)\n",
    "    print(f\"  ✓ Final OOF AUC: {auc_final:.5f}\")\n",
    "\n",
    "    # ===== SAVE SUBMISSION =====\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"SAVING SUBMISSION\")\n",
    "    print(f\"{'='*60}\")\n",
    "    submission = pd.DataFrame({ID_COL: test_ids, TARGET_COL: test_prob})\n",
    "    submission.to_csv(\"submission.csv\", index=False)\n",
    "    print(f\"✓ Saved submission.csv\")\n",
    "    print(f\"  Shape: {submission.shape}\")\n",
    "    print(f\"  Predicted prob range: [{test_prob.min():.4f}, {test_prob.max():.4f}]\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 14262372,
     "sourceId": 91722,
     "sourceType": "competition"
    },
    {
     "datasetId": 902,
     "sourceId": 370089,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8678604,
     "sourceId": 13651350,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8678647,
     "sourceId": 13651405,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 8628.362547,
   "end_time": "2025-11-11T05:58:11.955483",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-11T03:34:23.592936",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
