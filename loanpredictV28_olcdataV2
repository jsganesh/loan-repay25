{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91722,"databundleVersionId":14262372,"sourceType":"competition"},{"sourceId":13651350,"sourceType":"datasetVersion","datasetId":8678604},{"sourceId":13651405,"sourceType":"datasetVersion","datasetId":8678647}],"dockerImageVersionId":31153,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# filename: lp28_olcdatav2.py\n# Kaggle S5E11 â€” Loan Payback (CPU-Optimized, Blended Solution with External Data)\n# Description: This is the final, robust version designed to achieve a top score.\n#              It fixes the KeyError and uses a CPU-optimized pipeline to blend models\n#              enriched with powerful features from the original Lending Club dataset.\n\n# --- HOW TO USE ---\n# 1. In your Kaggle Notebook, click \"Add data\" in the right-hand panel.\n# 2. Search for \"Lending Club Loan Data\" and add it to your notebook.\n# 3. The script will automatically find the data at the path \"/kaggle/input/lending-club-loan-data/...\"\n# 4. If running locally, download the dataset and place it in a folder named \"lending-club-loan-data\".\n\nimport os\nimport warnings\nfrom typing import List, Tuple, Dict, Any\nimport random\nimport gc\n\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nimport catboost as cb\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.isotonic import IsotonicRegression\n\nwarnings.filterwarnings(\"ignore\")\n\nclass Config:\n    # Core settings\n    TARGET_COL = \"loan_paid_back\"\n    ID_COL = \"id\"\n    SEED = 42\n    N_FOLDS = 7\n    \n    # Data paths\n    TRAIN_PATH = \"train.csv\" if os.path.exists(\"train.csv\") else \"/kaggle/input/playground-series-s5e11/train.csv\"\n    TEST_PATH  = \"test.csv\"  if os.path.exists(\"test.csv\")  else \"/kaggle/input/playground-series-s5e11/test.csv\"\n    \n    SEEDS = [42, 2025, 777, 131, 902, 111, 313, 501]\n\n    # Ensemble & Model settings\n    N_MODELS_PER_FOLD_LGBM = 5\n    \n    # --- FIX: Removed 'home_ownership' as it's not in the competition data ---\n    AGG_COLS = [\"grade_letter\", \"loan_purpose\", \"employment_status\"]\n    NUM_COLS = [\"annual_income\", \"debt_to_income_ratio\", \"credit_score\", \"loan_amount\", \"interest_rate\"]\n    \n    # Blending weights\n    LGBM_WEIGHT = 0.6\n    CATBOOST_WEIGHT = 0.4\n\ndef load_data(cfg: Config) -> Tuple[pd.DataFrame, pd.Series, pd.DataFrame, pd.Series]:\n    print(\"Loading data...\")\n    train = pd.read_csv(cfg.TRAIN_PATH)\n    test = pd.read_csv(cfg.TEST_PATH)\n    y = train[cfg.TARGET_COL].astype(float)\n    X = train.drop(columns=[cfg.TARGET_COL])\n    test_ids = test[cfg.ID_COL].copy()\n    return X, y, test, test_ids\n\ndef initial_feature_engineering(df: pd.DataFrame) -> pd.DataFrame:\n    out = df.copy()\n    for c in [\"annual_income\", \"debt_to_income_ratio\", \"credit_score\", \"loan_amount\", \"interest_rate\"]:\n        out[c] = pd.to_numeric(out[c], errors=\"coerce\")\n    \n    out['grade_letter'] = out['grade_subgrade'].str[0]\n    out[\"annual_income_log\"] = np.log1p(out[\"annual_income\"])\n    out[\"loan_to_income\"] = out[\"loan_amount\"] / (out[\"annual_income\"] + 1e-6)\n    out[\"dti_x_int\"] = out[\"debt_to_income_ratio\"] * out[\"interest_rate\"]\n    \n    return out\n\ndef generate_refined_features(train_df: pd.DataFrame, test_df: pd.DataFrame, cfg: Config) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    print(\"Generating refined aggregation features...\")\n    train_df['source'] = 'train'\n    test_df['source'] = 'test'\n    df = pd.concat([train_df, test_df], axis=0, ignore_index=True)\n    \n    for cat_col in cfg.AGG_COLS:\n        for num_col in cfg.NUM_COLS:\n            aggs = df.groupby(cat_col)[num_col].agg(['mean', 'std'])\n            aggs.columns = [f'{cat_col}_{num_col}_{agg}' for agg in aggs.columns]\n            df = df.join(aggs, on=cat_col)\n            df[f'{num_col}_vs_{cat_col}_mean'] = df[num_col] - df[f'{cat_col}_{num_col}_mean']\n\n    train_refined = df[df['source'] == 'train'].drop('source', axis=1)\n    test_refined = df[df['source'] == 'test'].drop('source', axis=1)\n    \n    numeric_cols = train_refined.select_dtypes(include=np.number).columns\n    train_median = train_refined[numeric_cols].median()\n    train_refined = train_refined.fillna(train_median)\n    test_refined = test_refined.fillna(train_median)\n    \n    return train_refined, test_refined\n\ndef add_external_features(train_df: pd.DataFrame, test_df: pd.DataFrame, cfg: Config) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Loads the original Lending Club dataset to create and merge powerful aggregate features.\n    \"\"\"\n    print(\"\\n--- Generating and Merging Features from Original Lending Club Data ---\")\n    ext_path = \"/kaggle/input/lending-club-loan-data/accepted_2007_to_2018Q4.csv\"\n    if not os.path.exists(ext_path):\n        print(\"Original Lending Club data not found. Skipping this step.\")\n        return train_df, test_df\n        \n    use_cols = ['int_rate', 'annual_inc', 'dti', 'grade', 'sub_grade', 'home_ownership']\n    df_ext = pd.read_csv(ext_path, usecols=use_cols, low_memory=True).dropna()\n\n    df_ext['grade_subgrade_ext'] = df_ext['grade'] + df_ext['sub_grade']\n\n    subgrade_agg = df_ext.groupby('grade_subgrade_ext')[['int_rate', 'dti', 'annual_inc']].mean()\n    subgrade_agg.columns = [f'ext_subgrade_{col}_mean' for col in subgrade_agg.columns]\n\n    home_agg = df_ext.groupby('home_ownership')[['int_rate', 'annual_inc']].mean()\n    home_agg.columns = [f'ext_home_{col}_mean' for col in home_agg.columns]\n    \n    train_df = train_df.merge(subgrade_agg, left_on='grade_subgrade', right_index=True, how='left')\n    test_df = test_df.merge(subgrade_agg, left_on='grade_subgrade', right_index=True, how='left')\n    \n    # The competition data does not have 'home_ownership', so we need to add it first to merge\n    # This step is tricky; we will assume the external data can provide it\n    # A safer merge requires a common key not available here, so we will skip this for robustness\n    # train_df = train_df.merge(home_agg, on='home_ownership', how='left') # This would fail\n    # test_df = test_df.merge(home_agg, on='home_ownership', how='left')  # This would fail\n\n    fill_cols = list(subgrade_agg.columns) # + list(home_agg.columns)\n    for col in fill_cols:\n        median_val = train_df[col].median()\n        train_df[col].fillna(median_val, inplace=True)\n        test_df[col].fillna(median_val, inplace=True)\n        \n    print(f\"Successfully added {len(fill_cols)} new features from external data.\")\n    return train_df, test_df\n\ndef to_rank(a: np.ndarray) -> np.ndarray:\n    return a.argsort().argsort().astype(float) / len(a)\n\ndef train_and_evaluate_lgbm(X, y, T, cfg: Config) -> Tuple[np.ndarray, np.ndarray]:\n    print(\"\\n--- Starting LightGBM Training (Optimized) ---\")\n    \n    cat_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n    for col in cat_cols:\n        X[col] = X[col].astype('category')\n        T[col] = T[col].astype('category')\n        \n    common_cols = list(set(X.columns) & set(T.columns))\n    X, T = X[common_cols], T[common_cols]\n\n    base_lgbm_params = {\n        \"objective\": \"binary\", \"metric\": \"auc\", \"boosting_type\": \"gbdt\",\n        \"n_estimators\": 2500, \"learning_rate\": 0.03, \"num_leaves\": 40,\n        \"feature_fraction\": 0.8, \"bagging_fraction\": 0.8, \"bagging_freq\": 1,\n        \"lambda_l2\": 2.0, \"verbosity\": -1, \"n_jobs\": -1,\n    }\n    \n    param_cycle = []\n    for seed in cfg.SEEDS:\n        for leaves in [31, 41, 51, 61, 71]:\n            for feature_frac in [0.75, 0.8, 0.85]:\n                params = base_lgbm_params.copy()\n                params['random_state'] = seed\n                params['num_leaves'] = leaves\n                params['feature_fraction'] = feature_frac\n                param_cycle.append(params)\n    \n    random.seed(cfg.SEED)\n    random.shuffle(param_cycle)\n    \n    skf = StratifiedKFold(n_splits=cfg.N_FOLDS, shuffle=True, random_state=cfg.SEED)\n    oof_preds, test_preds_list = np.zeros(len(X)), []\n\n    for fold, (tr_idx, val_idx) in enumerate(skf.split(X, y), start=1):\n        print(f\"\\nFold {fold}/{cfg.N_FOLDS}\")\n        X_tr, y_tr, X_val, y_val = X.iloc[tr_idx], y.iloc[tr_idx], X.iloc[val_idx], y.iloc[val_idx]\n        \n        fold_val_ranks, fold_test_ranks = [], []\n        param_subset = param_cycle[(fold-1)*cfg.N_MODELS_PER_FOLD_LGBM : fold*cfg.N_MODELS_PER_FOLD_LGBM]\n        \n        for i, params in enumerate(param_subset):\n            model = lgb.LGBMClassifier(**params)\n            model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], eval_metric=\"auc\", callbacks=[lgb.early_stopping(100, verbose=False)])\n            val_preds = model.predict_proba(X_val)[:, 1]\n            test_preds = model.predict_proba(T)[:, 1]\n            fold_val_ranks.append(to_rank(val_preds))\n            fold_test_ranks.append(to_rank(test_preds))\n            print(f\"  LGBM Model {i+1} (leaves={params['num_leaves']}): Val AUC = {roc_auc_score(y_val, val_preds):.5f}\")\n\n        oof_preds[val_idx] = np.mean(fold_val_ranks, axis=0)\n        test_preds_list.append(np.mean(fold_test_ranks, axis=0))\n        print(f\"  Fold {fold} LGBM Rank-Averaged AUC: {roc_auc_score(y_val, oof_preds[val_idx]):.5f}\")\n\n    avg_test_preds = np.mean(test_preds_list, axis=0)\n    return oof_preds, avg_test_preds\n\ndef train_and_evaluate_catboost(X, y, T, cfg: Config) -> Tuple[np.ndarray, np.ndarray]:\n    print(\"\\n--- Starting CatBoost Training (CPU-Optimized) ---\")\n    \n    cat_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n    common_cols = list(set(X.columns) & set(T.columns))\n    X, T = X[common_cols].copy(), T[common_cols].copy()\n\n    for col in cat_cols:\n        X[col], T[col] = X[col].astype(str).fillna('__MISSING__'), T[col].astype(str).fillna('__MISSING__')\n\n    cb_params = {\n        'objective': 'Logloss', 'eval_metric': 'AUC', 'iterations': 1500,\n        'learning_rate': 0.05, 'depth': 6, 'random_seed': cfg.SEED,\n        'verbose': 0, 'allow_writing_files': False, 'thread_count': -1\n    }\n    \n    skf = StratifiedKFold(n_splits=cfg.N_FOLDS, shuffle=True, random_state=cfg.SEED)\n    oof_preds, test_preds_list = np.zeros(len(X)), []\n    \n    for fold, (tr_idx, val_idx) in enumerate(skf.split(X, y), start=1):\n        print(f\"\\nFold {fold}/{cfg.N_FOLDS}\")\n        X_tr, y_tr, X_val, y_val = X.iloc[tr_idx], y.iloc[tr_idx], X.iloc[val_idx], y.iloc[val_idx]\n        \n        model = cb.CatBoostClassifier(**cb_params)\n        model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], cat_features=cat_cols, early_stopping_rounds=100, verbose=False)\n        val_preds = model.predict_proba(X_val)[:, 1]\n        test_preds = model.predict_proba(T)[:, 1]\n        \n        oof_preds[val_idx] = val_preds\n        test_preds_list.append(test_preds)\n        print(f\"  Fold {fold} CatBoost AUC: {roc_auc_score(y_val, val_preds):.5f}\")\n\n    avg_test_preds = np.mean(test_preds_list, axis=0)\n    return oof_preds, avg_test_preds\n\ndef main():\n    cfg = Config()\n    \n    X_raw, y, T_raw, test_ids = load_data(cfg)\n    \n    X_fe, T_fe = initial_feature_engineering(X_raw), initial_feature_engineering(T_raw)\n    \n    X_refined, T_refined = generate_refined_features(X_fe, T_fe, cfg)\n    \n    X_final, T_final = add_external_features(X_refined, T_refined, cfg)\n    \n    gc.collect()\n    \n    oof_lgbm, test_lgbm = train_and_evaluate_lgbm(X_final.copy(), y, T_final.copy(), cfg)\n    oof_cb, test_cb = train_and_evaluate_catboost(X_final.copy(), y, T_final.copy(), cfg)\n\n    oof_blend = cfg.LGBM_WEIGHT * oof_lgbm + cfg.CATBOOST_WEIGHT * to_rank(oof_cb)\n    test_blend = cfg.LGBM_WEIGHT * test_lgbm + cfg.CATBOOST_WEIGHT * to_rank(test_cb)\n\n    oof_auc_lgbm, oof_auc_cb, oof_auc_blend = roc_auc_score(y, oof_lgbm), roc_auc_score(y, oof_cb), roc_auc_score(y, oof_blend)\n    \n    print(\"\\n--- Final OOF Scores ---\")\n    print(f\"LGBM Rank-Averaged OOF AUC: {oof_auc_lgbm:.5f}\")\n    print(f\"CatBoost OOF AUC:           {oof_auc_cb:.5f}\")\n    print(f\"Blended OOF AUC:            {oof_auc_blend:.5f}\")\n\n    print(\"\\nCalibrating blended predictions...\")\n    iso = IsotonicRegression(out_of_bounds=\"clip\").fit(oof_blend, y.values)\n    calibrated_preds = np.clip(iso.predict(test_blend), 0.0, 1.0)\n    \n    print(\"Creating submission file...\")\n    submission = pd.DataFrame({cfg.ID_COL: test_ids, cfg.TARGET_COL: calibrated_preds})\n    submission.to_csv(\"submission.csv\", index=False)\n    print(\"\\nSaved submission.csv successfully!\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}